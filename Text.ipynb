{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.tsv\", delimiter = '\\t')\n",
    "test = pd.read_csv(\"test.tsv\", delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "5         6           1  of escapades demonstrating the adage that what...   \n",
       "6         7           1                                                 of   \n",
       "7         8           1  escapades demonstrating the adage that what is...   \n",
       "8         9           1                                          escapades   \n",
       "9        10           1  demonstrating the adage that what is good for ...   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  \n",
       "5          2  \n",
       "6          2  \n",
       "7          2  \n",
       "8          2  \n",
       "9          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b390205978>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [WordNetLemmatizer().lemmatize(w) for w in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\neloy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\neloy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "vectorizer_w = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = None,ngram_range = (1,3), analyzer = 'word', encoding = 'utf-8', tokenizer = LemmaTokenizer())\n",
    "vectorizer_c = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = None,ngram_range = (2,6), analyzer = 'char', encoding = 'utf-8', tokenizer = LemmaTokenizer())\n",
    "X_train_w = vectorizer_w.fit_transform(train['Phrase'])\n",
    "X_train_c = vectorizer_c.fit_transform(train['Phrase'])\n",
    "X_test_w = vectorizer_w.transform(test['Phrase'])\n",
    "X_test_c = vectorizer_c.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sparse.hstack([X_train_w, X_train_c])\n",
    "X_test = sparse.hstack([X_test_w, X_test_c])\n",
    "\n",
    "#Tried Oversampling methods using imbalanced-learn API(http://contrib.scikit-learn.org/imbalanced-learn/stable/api.html)\n",
    "#However Oversampling did not help\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "ada = ADASYN(random_state=152)\n",
    "#X_train_ros, y_train_ros = ros.fit_sample(X_train, y_train)\n",
    "#X_train_ada, y_train_ada = ada.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in Train dataset i.e. n_samples: 156060, Number of features in Train dataset i.e. n_features: 545680\n",
      "Number of samples in Test dataset i.e. n_samples: 66292, Number of features in Test dataset i.e. n_features: 545680\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples in Train dataset i.e. n_samples: %d, Number of features in Train dataset i.e. n_features: %d\" % X_train.shape)\n",
    "print(\"Number of samples in Test dataset i.e. n_samples: %d, Number of features in Test dataset i.e. n_features: %d\" % X_test.shape)\n",
    "print(\"\\n\")\n",
    "#print(\"Number of samples in Resample Train dataset(Ramdom Sampler) i.e. n_samples: %d, Number of features in Train dataset i.e. n_features: %d\" % X_train_ros.shape)\n",
    "#print(\"Number of samples in Resample Train dataset(ADASYN) i.e. n_samples: %d, Number of features in Train dataset i.e. n_features: %d\" % X_train_ada.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_nb = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(\"SampleSubmission.csv\",sep='delimiter', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_NB.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 11644 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 194.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 194.1min finished\n"
     ]
    }
   ],
   "source": [
    "lclf = LogisticRegression(solver = 'saga',multi_class = 'multinomial', max_iter = 4000, \n",
    "                          C = 4, random_state = 42, verbose = 10, class_weight = 'balanced')\n",
    "\n",
    "#parameters = {'C':[2 , 4] }\n",
    "#scorer = make_scorer(accuracy_score)\n",
    "#cv = StratifiedShuffleSplit(2, random_state = 62)\n",
    "#grid_obj = GridSearchCV(lclf, param_grid=parameters, cv = cv, scoring=scorer, n_jobs=-1, verbose=10)\n",
    "#grid_fit = grid_obj.fit(X_train, y_train)\n",
    "#best_clf = grid_fit.best_estimator_\n",
    "\n",
    "predictions = (lclf.fit(X_train, y_train)).predict(X_test)\n",
    "#best_predictions = best_clf.predict(X_tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard NLP Pre-Processing\n",
    "X_train = train['Phrase']\n",
    "X_test = test['Phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A series of escapades demonstrating the adage ...\n",
       "1    A series of escapades demonstrating the adage ...\n",
       "2                                             A series\n",
       "3                                                    A\n",
       "4                                               series\n",
       "Name: Phrase, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    a series of escapades demonstrating the adage ...\n",
       "1    a series of escapades demonstrating the adage ...\n",
       "2                                             a series\n",
       "3                                                    a\n",
       "4                                               series\n",
       "Name: Phrase, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NORMALIZATION - Converting to lower case\n",
    "X_train_l = X_train.str.lower()\n",
    "print(X_train_l[0])\n",
    "X_train_l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of escapades demonstrating the adage that what is good for the goose is also good for the gander   some of which occasionally amuses but none of which amounts to much of a story  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    a series of escapades demonstrating the adage ...\n",
       "1    a series of escapades demonstrating the adage ...\n",
       "2                                             a series\n",
       "3                                                    a\n",
       "4                                               series\n",
       "Name: Phrase, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NORMALIZATION - Removing Punctuation marks\n",
    "import re\n",
    "def punc_rem(y):\n",
    "    return re.sub(r\"[^a-zA-Z0-9]\", \" \", y)\n",
    "X_train_p = X_train_l.apply(lambda x: punc_rem(x))\n",
    "print(X_train_p[0])\n",
    "X_train_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gander', 'some', 'of', 'which', 'occasionally', 'amuses', 'but', 'none', 'of', 'which', 'amounts', 'to', 'much', 'of', 'a', 'story']\n",
      "0    [a, series, of, escapades, demonstrating, the,...\n",
      "1    [a, series, of, escapades, demonstrating, the,...\n",
      "2                                          [a, series]\n",
      "3                                                  [a]\n",
      "4                                             [series]\n",
      "Name: Phrase, dtype: object\n",
      "['a series of escapades demonstrating the adage that what is good for the goose is also good for the gander   some of which occasionally amuses but none of which amounts to much of a story']\n",
      "0    [a series of escapades demonstrating the adage...\n",
      "1    [a series of escapades demonstrating the adage...\n",
      "2                                           [a series]\n",
      "3                                                  [a]\n",
      "4                                             [series]\n",
      "Name: Phrase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#TOKENIZATION - Word & Setence tokenizers\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "X_train_wt = X_train_p.apply(lambda x : word_tokenize(x))\n",
    "X_train_st = X_train_p.apply(lambda x : sent_tokenize(x))\n",
    "\n",
    "print(X_train_wt[0])\n",
    "print(X_train_wt.head())\n",
    "print(X_train_st[0])\n",
    "print(X_train_st.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neloy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['series', 'escapades', 'demonstrating', 'adage', 'good', 'goose', 'also', 'good', 'gander', 'occasionally', 'amuses', 'none', 'amounts', 'much', 'story']\n",
      "[]\n",
      "0    [series, escapades, demonstrating, adage, good...\n",
      "1    [series, escapades, demonstrating, adage, good...\n",
      "2                                             [series]\n",
      "3                                                   []\n",
      "4                                             [series]\n",
      "Name: Phrase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#STOPWORDS Removal\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "def stop_words(x):\n",
    "    return [i for i in x if i not in stopwords.words('english')]\n",
    "X_train_sw = X_train_wt.apply(lambda x : stop_words(x))\n",
    "print(X_train_sw[0])\n",
    "print(X_train_sw[3])\n",
    "print(X_train_sw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\neloy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'DT'), ('series', 'NN'), ('of', 'IN'), ('escapades', 'NNS'), ('demonstrating', 'VBG'), ('the', 'DT'), ('adage', 'NN'), ('that', 'IN'), ('what', 'WP'), ('is', 'VBZ'), ('good', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('goose', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('good', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('gander', 'NN'), ('some', 'DT'), ('of', 'IN'), ('which', 'WDT'), ('occasionally', 'RB'), ('amuses', 'VBZ'), ('but', 'CC'), ('none', 'NN'), ('of', 'IN'), ('which', 'WDT'), ('amounts', 'NNS'), ('to', 'TO'), ('much', 'JJ'), ('of', 'IN'), ('a', 'DT'), ('story', 'NN')]\n",
      "[('a', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "#POS (Parts Of Speech Tagging) & NER (Named Entity Recognition)\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def postag(x):\n",
    "    return nltk.pos_tag(x)\n",
    "X_train_pos = X_train_wt.apply(lambda x: postag(x))\n",
    "print(X_train_pos[0])\n",
    "print(X_train_pos[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\neloy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'DT'): \n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('series', 'NN'): \n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('of', 'IN'): \n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('escapades', 'NNS'): \n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "('demonstrating', 'VBG'): \n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "('the', 'DT'): \n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('adage', 'NN'): \n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('that', 'IN'): \n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('what', 'WP'): \n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "('is', 'VBZ'): \n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "('good', 'JJ'): \n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "('for', 'IN'): \n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('the', 'DT'): \n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('goose', 'NN'): \n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('is', 'VBZ'): \n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "('also', 'RB'): \n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "('good', 'JJ'): \n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "('for', 'IN'): \n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('the', 'DT'): \n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('gander', 'NN'): \n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('some', 'DT'): \n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('of', 'IN'): \n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('which', 'WDT'): \n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "('occasionally', 'RB'): \n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "('amuses', 'VBZ'): \n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "('but', 'CC'): \n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "('none', 'NN'): \n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('of', 'IN'): \n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('which', 'WDT'): \n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "('amounts', 'NNS'): \n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "('to', 'TO'): \n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "('much', 'JJ'): \n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "('of', 'IN'): \n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('a', 'DT'): \n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('story', 'NN'): \n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "#nltk.help.upenn_tagset('CC')\n",
    "nltk.download('tagsets')\n",
    "for i in X_train_pos[0]:\n",
    "    print(\"{}: \".format(i))\n",
    "    (nltk.help.upenn_tagset(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords Corpus\\n\\nThis corpus contains lists of stop words for several languages.  These\\nare high-frequency grammatical words which are usually ignored in text\\nretrieval applications.\\n\\nThey were obtained from:\\nhttp://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/\\n\\nThe stop words for the Romanian language were obtained from:\\nhttp://arlc.ro/resources/\\n\\nThe English list has been augmented\\nhttps://github.com/nltk/nltk_data/issues/22\\n\\nThe German list has been corrected\\nhttps://github.com/nltk/nltk_data/pull/49\\n\\nA Kazakh list has been added\\nhttps://github.com/nltk/nltk_data/pull/52\\n\\nA Nepali list has been added\\nhttps://github.com/nltk/nltk_data/pull/83\\n\\nAn Azerbaijani list has been added\\nhttps://github.com/nltk/nltk_data/pull/100\\n\\nA Greek list has been added\\nhttps://github.com/nltk/nltk_data/pull/103\\n\\nAn Indonesian list has been added\\nhttps://github.com/nltk/nltk_data/pull/112\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.readme() #https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\neloy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\neloy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  a/DT\n",
      "  series/NN\n",
      "  of/IN\n",
      "  escapades/NNS\n",
      "  demonstrating/VBG\n",
      "  the/DT\n",
      "  adage/NN\n",
      "  that/IN\n",
      "  what/WP\n",
      "  is/VBZ\n",
      "  good/JJ\n",
      "  for/IN\n",
      "  the/DT\n",
      "  goose/NN\n",
      "  is/VBZ\n",
      "  also/RB\n",
      "  good/JJ\n",
      "  for/IN\n",
      "  the/DT\n",
      "  gander/NN\n",
      "  some/DT\n",
      "  of/IN\n",
      "  which/WDT\n",
      "  occasionally/RB\n",
      "  amuses/VBZ\n",
      "  but/CC\n",
      "  none/NN\n",
      "  of/IN\n",
      "  which/WDT\n",
      "  amounts/NNS\n",
      "  to/TO\n",
      "  much/JJ\n",
      "  of/IN\n",
      "  a/DT\n",
      "  story/NN)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "def ner(x):\n",
    "    return nltk.ne_chunk(x)\n",
    "\n",
    "X_train_ner = X_train_pos.apply(lambda x: ner(x))\n",
    "\n",
    "print(X_train_ner[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (GPE Bangladesh/NNP) is/VBZ a/DT great/JJ country/NN)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(nltk.pos_tag(word_tokenize(\"Bangladesh is a great country\"))))\n",
    "#https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A series of escapades demonstrating the adage that what is good for the goose\n",
      "['a', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose']\n",
      "[('a', 'DT'), ('series', 'NN'), ('of', 'IN'), ('escapades', 'NNS'), ('demonstrating', 'VBG'), ('the', 'DT'), ('adage', 'NN'), ('that', 'IN'), ('what', 'WP'), ('is', 'VBZ'), ('good', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('goose', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#CFG - Context Free Grammer\n",
    "\n",
    "print(X_train[1])\n",
    "print(X_train_wt[1])\n",
    "print(X_train_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Chart.parses at 0x000001B3CFF0D0F8>\n"
     ]
    }
   ],
   "source": [
    "custom_grammer = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP \n",
    "VP -> V NP | VP PP | JJ\n",
    "Det -> 'the'|'a'\n",
    "N -> 'series'|'escapades'|'adage'|'goose'\n",
    "V -> 'demonstrating'|'is'\n",
    "JJ -> 'good'\n",
    "P -> 'that'|'for'|'of'|'what'\n",
    "\"\"\")\n",
    "\n",
    "custom_parser = nltk.ChartParser(custom_grammer)\n",
    "print(custom_parser.parse(X_train_wt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChartParser in module nltk.parse.chart:\n",
      "\n",
      "class ChartParser(nltk.parse.api.ParserI)\n",
      " |  A generic chart parser.  A \"strategy\", or list of\n",
      " |  ``ChartRuleI`` instances, is used to decide what edges to add to\n",
      " |  the chart.  In particular, ``ChartParser`` uses the following\n",
      " |  algorithm to parse texts:\n",
      " |  \n",
      " |  | Until no new edges are added:\n",
      " |  |   For each *rule* in *strategy*:\n",
      " |  |     Apply *rule* to any applicable edges in the chart.\n",
      " |  | Return any complete parses in the chart\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ChartParser\n",
      " |      nltk.parse.api.ParserI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, grammar, strategy=[<nltk.parse.chart.LeafInitRule object at 0x000001B38FB47DD8>, <nltk.parse.chart.EmptyPredictRule object at 0x000001B38FB47E10>, <nltk.parse.chart.BottomUpPredictCombineRule object at 0x000001B38FB47E48>, <nltk.parse.chart.SingleEdgeFundamentalRule object at 0x000001B38FB47E80>], trace=0, trace_chart_width=50, use_agenda=True, chart_class=<class 'nltk.parse.chart.Chart'>)\n",
      " |      Create a new chart parser, that uses ``grammar`` to parse\n",
      " |      texts.\n",
      " |      \n",
      " |      :type grammar: CFG\n",
      " |      :param grammar: The grammar used to parse texts.\n",
      " |      :type strategy: list(ChartRuleI)\n",
      " |      :param strategy: A list of rules that should be used to decide\n",
      " |          what edges to add to the chart (top-down strategy by default).\n",
      " |      :type trace: int\n",
      " |      :param trace: The level of tracing that should be used when\n",
      " |          parsing a text.  ``0`` will generate no tracing output;\n",
      " |          and higher numbers will produce more verbose tracing\n",
      " |          output.\n",
      " |      :type trace_chart_width: int\n",
      " |      :param trace_chart_width: The default total width reserved for\n",
      " |          the chart in trace output.  The remainder of each line will\n",
      " |          be used to display edges.\n",
      " |      :type use_agenda: bool\n",
      " |      :param use_agenda: Use an optimized agenda-based algorithm,\n",
      " |          if possible.\n",
      " |      :param chart_class: The class that should be used to create\n",
      " |          the parse charts.\n",
      " |  \n",
      " |  chart_parse(self, tokens, trace=None)\n",
      " |      Return the final parse ``Chart`` from which all possible\n",
      " |      parse trees can be extracted.\n",
      " |      \n",
      " |      :param tokens: The sentence to be parsed\n",
      " |      :type tokens: list(str)\n",
      " |      :rtype: Chart\n",
      " |  \n",
      " |  grammar(self)\n",
      " |      :return: The grammar used by this parser.\n",
      " |  \n",
      " |  parse(self, tokens, tree_class=<class 'nltk.tree.Tree'>)\n",
      " |      :return: An iterator that generates parse trees for the sentence.\n",
      " |      When possible this list is sorted from most likely to least likely.\n",
      " |      \n",
      " |      :param sent: The sentence to be parsed\n",
      " |      :type sent: list(str)\n",
      " |      :rtype: iter(Tree)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.parse.api.ParserI:\n",
      " |  \n",
      " |  parse_all(self, sent, *args, **kwargs)\n",
      " |      :rtype: list(Tree)\n",
      " |  \n",
      " |  parse_one(self, sent, *args, **kwargs)\n",
      " |      :rtype: Tree or None\n",
      " |  \n",
      " |  parse_sents(self, sents, *args, **kwargs)\n",
      " |      Apply ``self.parse()`` to each element of ``sents``.\n",
      " |      :rtype: iter(iter(Tree))\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.parse.api.ParserI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.ChartParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for custom_tree in custom_parser.parse(X_train_wt[1]):\n",
    "    print(\"Sasikanth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom grammar\n",
    "my_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(my_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-73bb262fb306>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.pos_tag(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<generator object Chart.parses at 0x000001B3BE9A9728>\n",
      "<class 'nltk.tree.Tree'>\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "<class 'nltk.tree.Tree'>\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "# Parse a sentence\n",
    "sentence = word_tokenize(\"I shot an elephant in my pajamas\")\n",
    "print(type(sentence))\n",
    "nltk.pos_tag(sentence)\n",
    "print(parser.parse(sentence))\n",
    "for tree in parser.parse(sentence):\n",
    "    print(type(tree))\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gander', 'some', 'of', 'which', 'occasionally', 'amuses', 'but', 'none', 'of', 'which', 'amounts', 'to', 'much', 'of', 'a', 'story']\n",
      "['a', 'seri', 'of', 'escapad', 'demonstr', 'the', 'adag', 'that', 'what', 'is', 'good', 'for', 'the', 'goos', 'is', 'also', 'good', 'for', 'the', 'gander', 'some', 'of', 'which', 'occasion', 'amus', 'but', 'none', 'of', 'which', 'amount', 'to', 'much', 'of', 'a', 'stori']\n"
     ]
    }
   ],
   "source": [
    "#STEMMING and LEMMATIZATION\n",
    "from nltk.stem import porter\n",
    "stemmer = porter.PorterStemmer()\n",
    "def stmr(x):\n",
    "    return [stemmer.stem(i) for i in x]\n",
    "X_train_stm = X_train_wt.apply(lambda x: stmr(x))\n",
    "print(X_train_wt[0])\n",
    "print(X_train_stm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lmtr(x):\n",
    "    return [WordNetLemmatizer().lemmatize(i) for i in x]\n",
    "\n",
    "def lmtrv(x):\n",
    "    return [WordNetLemmatizer().lemmatize(i, pos = 'v') for i in x]\n",
    "\n",
    "X_train_lm = X_train_wt.apply(lambda x: lmtr(x))\n",
    "X_train_lmv = X_train_wt.apply(lambda x: lmtrv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose']\n",
      "['a', 'series', 'of', 'escapade', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose']\n",
      "['a', 'series', 'of', 'escapades', 'demonstrate', 'the', 'adage', 'that', 'what', 'be', 'good', 'for', 'the', 'goose']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_wt[1])\n",
    "print(X_train_lm[1])\n",
    "print(X_train_lmv[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
